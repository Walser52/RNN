{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c76adf30-8636-432d-a46e-e287826ce24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfa8788-9378-4730-b0cb-7b0e4ad7fce7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 1: Create Dataset¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e33e3978-0842-413f-8f49-e528cd2969cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class batteriesDS(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        \n",
    "        self.data = torch.tensor(data.values)\n",
    "        self.targets = torch.tensor(targets.values)\n",
    "                \n",
    "        return\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        current_sample = self.data[index,:]\n",
    "        current_targets = self.targets[index]\n",
    "        return {\n",
    "            \"x\": torch.tensor(current_sample, dtype = torch.float),\n",
    "            \"y\": torch.tensor(current_targets, dtype = torch.float)\n",
    "        }\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "badd3d3c-2d55-4c5a-848b-f8dce273e1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cycle', 'Voltages', 'C_rate', 'D_rate', 'Tem', 'Capacity',\n",
       "       'I_63perdown(mA)_cv', 'I_gap(mA)_cv', 'V_63perup(V)_cc', 'V_gap(V)_cc',\n",
       "       'file', 'intercept_rv', 'pval_rv', 'rval_rv', 'slope_rv', 'stderr_rv',\n",
       "       't_gap63perdown_cv', 't_gap63perup_cc', 'time_cc', 'time_cv'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read file\n",
    "\n",
    "csv = r\"D:\\GIK - R&D - Data\\Data\\Capacity estimation of LiBs\\Dataset_3_NCM_NCA_battery-customized.csv\"\n",
    "df = pd.read_csv(csv)\n",
    "df.columns\n",
    "#df = pd.read_csv(csv, usecols =['C_rate','D_rate','Tem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "bf49904d-b498-45a3-82e0-b4bb6cb5e7c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I_63perdown(mA)_cv</th>\n",
       "      <th>I_gap(mA)_cv</th>\n",
       "      <th>V_63perup(V)_cc</th>\n",
       "      <th>V_gap(V)_cc</th>\n",
       "      <th>intercept_rv</th>\n",
       "      <th>pval_rv</th>\n",
       "      <th>rval_rv</th>\n",
       "      <th>slope_rv</th>\n",
       "      <th>stderr_rv</th>\n",
       "      <th>t_gap63perdown_cv</th>\n",
       "      <th>t_gap63perup_cc</th>\n",
       "      <th>time_cc</th>\n",
       "      <th>time_cv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>519.207253</td>\n",
       "      <td>1122.849409</td>\n",
       "      <td>3.731749</td>\n",
       "      <td>1.265946</td>\n",
       "      <td>4.192260</td>\n",
       "      <td>7.342098e-32</td>\n",
       "      <td>-0.955332</td>\n",
       "      <td>-0.000200</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>250.000026</td>\n",
       "      <td>3150.000329</td>\n",
       "      <td>6872.660717</td>\n",
       "      <td>7780.282812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>518.446603</td>\n",
       "      <td>1121.533761</td>\n",
       "      <td>3.730067</td>\n",
       "      <td>1.270278</td>\n",
       "      <td>4.192053</td>\n",
       "      <td>1.278060e-30</td>\n",
       "      <td>-0.950509</td>\n",
       "      <td>-0.000196</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>250.000026</td>\n",
       "      <td>3140.000328</td>\n",
       "      <td>6879.456718</td>\n",
       "      <td>7786.384812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>519.442841</td>\n",
       "      <td>1123.503931</td>\n",
       "      <td>3.729229</td>\n",
       "      <td>1.272010</td>\n",
       "      <td>4.192176</td>\n",
       "      <td>9.445477e-31</td>\n",
       "      <td>-0.951044</td>\n",
       "      <td>-0.000197</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>255.000027</td>\n",
       "      <td>3130.000326</td>\n",
       "      <td>6872.568717</td>\n",
       "      <td>7789.694813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>518.808211</td>\n",
       "      <td>1125.145321</td>\n",
       "      <td>3.730388</td>\n",
       "      <td>1.269411</td>\n",
       "      <td>4.192225</td>\n",
       "      <td>1.381081e-30</td>\n",
       "      <td>-0.950371</td>\n",
       "      <td>-0.000195</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>255.000027</td>\n",
       "      <td>3140.000328</td>\n",
       "      <td>6862.474716</td>\n",
       "      <td>7789.602813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>517.886624</td>\n",
       "      <td>1123.454963</td>\n",
       "      <td>3.731397</td>\n",
       "      <td>1.266576</td>\n",
       "      <td>4.192079</td>\n",
       "      <td>1.740770e-30</td>\n",
       "      <td>-0.949956</td>\n",
       "      <td>-0.000193</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>260.000027</td>\n",
       "      <td>3150.000329</td>\n",
       "      <td>6849.806714</td>\n",
       "      <td>7787.434812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   I_63perdown(mA)_cv  I_gap(mA)_cv  V_63perup(V)_cc  V_gap(V)_cc  \\\n",
       "0          519.207253   1122.849409         3.731749     1.265946   \n",
       "1          518.446603   1121.533761         3.730067     1.270278   \n",
       "2          519.442841   1123.503931         3.729229     1.272010   \n",
       "3          518.808211   1125.145321         3.730388     1.269411   \n",
       "4          517.886624   1123.454963         3.731397     1.266576   \n",
       "\n",
       "   intercept_rv       pval_rv   rval_rv  slope_rv  stderr_rv  \\\n",
       "0      4.192260  7.342098e-32 -0.955332 -0.000200   0.000008   \n",
       "1      4.192053  1.278060e-30 -0.950509 -0.000196   0.000008   \n",
       "2      4.192176  9.445477e-31 -0.951044 -0.000197   0.000008   \n",
       "3      4.192225  1.381081e-30 -0.950371 -0.000195   0.000008   \n",
       "4      4.192079  1.740770e-30 -0.949956 -0.000193   0.000008   \n",
       "\n",
       "   t_gap63perdown_cv  t_gap63perup_cc      time_cc      time_cv  \n",
       "0         250.000026      3150.000329  6872.660717  7780.282812  \n",
       "1         250.000026      3140.000328  6879.456718  7786.384812  \n",
       "2         255.000027      3130.000326  6872.568717  7789.694813  \n",
       "3         255.000027      3140.000328  6862.474716  7789.602813  \n",
       "4         260.000027      3150.000329  6849.806714  7787.434812  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select appropriate columns\n",
    "\n",
    "features = ['I_63perdown(mA)_cv','I_gap(mA)_cv','V_63perup(V)_cc','V_gap(V)_cc','intercept_rv','pval_rv','rval_rv','slope_rv','stderr_rv','t_gap63perdown_cv','t_gap63perup_cc','time_cc','time_cv',]\n",
    "\n",
    "\n",
    "df_data = df.loc[:,features]\n",
    "df_targets = df.loc[:, 'Capacity']\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ad62f016-98be-47ce-a4e8-ea3750d2c540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:\t 8582\n",
      "Train size:\t 6865 \n",
      "Test size:\t 1717\n",
      "<torch.utils.data.dataset.Subset object at 0x0000016A842215E0>\n"
     ]
    }
   ],
   "source": [
    "#Instantiate Dataset Object\n",
    "dataset = batteriesDS(df_data, df_targets)\n",
    "\n",
    "#Train Test split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "print(\"Dataset:\\t\", len(dataset))\n",
    "print(\"Train size:\\t\", len(train_dataset), \"\\nTest size:\\t\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "7b060889-20d5-456d-9fc2-26572183ebaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataset.Subset"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_mean = torch.mean(train_dataset, axis=0)\n",
    "X_std = torch.std(X_train, axis=0)\n",
    "X_train_man_stdzd = (X_train-X_mean)/X_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2bad32-e6dc-420d-9adc-609c3816afd1",
   "metadata": {},
   "source": [
    "# Step 2: Make Dataset Iterable¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a88c23b6-cd14-488a-bda0-881d60b54ea4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "#num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = 5 #int(num_epochs)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True , drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "55e7e80b-0cc1-4ead-810d-c6c304dfb0ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fes33\\AppData\\Local\\Temp\\ipykernel_5252\\1412074353.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"x\": torch.tensor(current_sample, dtype = torch.float),\n",
      "C:\\Users\\fes33\\AppData\\Local\\Temp\\ipykernel_5252\\1412074353.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"y\": torch.tensor(current_targets, dtype = torch.float)\n"
     ]
    }
   ],
   "source": [
    "shape = [100,1,13]\n",
    "for i, d in enumerate(train_loader):\n",
    "    #print(d)\n",
    "    #print(d['x'])\n",
    "    rs = torch.reshape(d['x'], shape)\n",
    "    #print(rs)\n",
    "    if i == 2:\n",
    "        break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490858da-0182-4cc9-b04c-9c468dfd5907",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fed13e8-8333-4c9e-87de-0bc425ee0b5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 3RNN: Create RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "617afa53-0f35-44b7-bbe1-541c2cbc5eaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # Building your RNN\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, input_dim)\n",
    "        # batch_dim = number of samples per batch\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
    "\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        # (layer_dim, batch_size, hidden_dim)\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # We need to detach the hidden state to prevent exploding/vanishing gradients\n",
    "        # This is part of truncated backpropagation through time (BPTT)\n",
    "        out, hn = self.rnn(x, h0.detach())\n",
    "\n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 28, 10\n",
    "        # out[:, -1, :] --> 100, 10 --> just want last time step hidden states! \n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out.size() --> 100, 10\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd21b8e-2040-41fb-9efe-f9343d0111b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 4RNN: Instantiate Model Class¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a55e095c-b62b-4a9d-97c0-b1a26d830307",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "\n",
    "input_dim = len(features)\n",
    "hidden_dim = 100\n",
    "layer_dim = 1\n",
    "output_dim = 1\n",
    "\n",
    "model = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eba207-34e6-463f-ad16-c0d9105184ed",
   "metadata": {},
   "source": [
    "## Step 5RNN: Instantiate Loss Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b6e5afbf-e11a-4a2f-8129-a0d9e6af8eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a738fb52-f7d5-44d5-834e-2cb09fadb75a",
   "metadata": {},
   "source": [
    "## Step 6RNN: Insatntiate Optimizer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e726c5c2-167c-457a-8383-477f76246436",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e53c375e-c9b3-44e7-b70d-f96f266bca3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters In-Depth¶\n",
    "\n",
    "#     Input to Hidden Layer Affine Function\n",
    "#         A1, B1\n",
    "#     Hidden Layer to Output Affine Function\n",
    "#         A2, B2\n",
    "#     Hidden Layer to Hidden Layer Affine Function\n",
    "#         A3, B3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "dff73722-f16c-4d03-bc26-f1eea87c439e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.parameters())) #We should have 6 groups as shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e99ef445-3b60-4403-a2c9-e95353d72f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 13])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input --> Hidden (A1)\n",
    "list(model.parameters())[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "398d5914-306e-4ae8-9f87-be4ee2fffc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input --> Hidden BIAS (B1)\n",
    "list(model.parameters())[2].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "faff2edd-e118-4446-a344-210ac26529ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 100])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hidden --> Hidden (A3)\n",
    "list(model.parameters())[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e00f3580-2015-42fa-b1ba-80514ae0823b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hidden --> Hidden BIAS(B3)\n",
    "list(model.parameters())[3].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0fb6af44-2f11-47e7-8e16-d88464644b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hidden --> Output BIAS (B2)\n",
    "list(model.parameters())[5].size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "40392fad-d3f5-48eb-88a9-b40e6ccddddf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hidden --> Output (A2)\n",
    "list(model.parameters())[4].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de479f70-b712-4f0c-bed4-5097524333d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 7RNN: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1815e508-98b2-49f3-84f0-d7790302dfd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of steps to unroll\n",
    "seq_dim = 1  \n",
    "shape = [batch_size,1,input_dim]\n",
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, trainbatch in enumerate(train_loader):\n",
    "        model.train()\n",
    "        # Load images as tensors with gradient accumulation abilities\n",
    "        #images = images.view(-1, seq_dim, input_dim).requires_grad_()\n",
    "        trainbatch['x'] = torch.reshape(trainbatch['x'], shape)\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        # outputs.size() --> 100, 10\n",
    "        outputs = model(trainbatch['x'])\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, trainbatch['y'])\n",
    "        \n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        #print(i)\n",
    "        \n",
    "        iter += 1\n",
    "        print(loss.item())\n",
    "#         if iter % 50 == 0:\n",
    "#             model.eval()\n",
    "#             # Calculate Accuracy         \n",
    "#             correct = 0\n",
    "#             total = 0\n",
    "#             # Iterate through test dataset\n",
    "#             for testbatch in test_loader: \n",
    "#                 # Load images to a Torch tensors with gradient accumulation abilities\n",
    "#                 #images = images.view(-1, seq_dim, input_dim)\n",
    "#                 testbatch['x']= torch.reshape(trainbatch['x'], shape)\n",
    "\n",
    "#                 # Forward pass only to get logits/output\n",
    "#                 outputs = model(testbatch['x'])\n",
    "\n",
    "#                 # Get predictions from the maximum value\n",
    "#                 _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "#                 # Total number of labels\n",
    "#                 total += testbatch['y'].size(0)\n",
    "\n",
    "#                 # Total correct predictions\n",
    "#                 correct += (predicted == testbatch['y']).sum()\n",
    "\n",
    "#             accuracy = 100 * correct / total\n",
    "\n",
    "#             # Print Loss\n",
    "#             print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "913249fd-d4b7-48ae-b180-497b53eaa080",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fes33\\AppData\\Local\\Temp\\ipykernel_5252\\1412074353.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"x\": torch.tensor(current_sample, dtype = torch.float),\n",
      "C:\\Users\\fes33\\AppData\\Local\\Temp\\ipykernel_5252\\1412074353.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"y\": torch.tensor(current_targets, dtype = torch.float)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'view'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [68]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load images as tensors with gradient accumulation abilities\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, seq_dim, input_dim)\u001b[38;5;241m.\u001b[39mrequires_grad_()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Clear gradients w.r.t. parameters\u001b[39;00m\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'view'"
     ]
    }
   ],
   "source": [
    "# # Original code\n",
    "\n",
    "\n",
    "# # Number of steps to unroll\n",
    "# seq_dim = 1  \n",
    "\n",
    "# iter = 0\n",
    "# for epoch in range(num_epochs):\n",
    "#     for i, (images, labels) in enumerate(train_loader):\n",
    "#         model.train()\n",
    "#         # Load images as tensors with gradient accumulation abilities\n",
    "#         images = images.view(-1, seq_dim, input_dim).requires_grad_()\n",
    "\n",
    "#         # Clear gradients w.r.t. parameters\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass to get output/logits\n",
    "#         # outputs.size() --> 100, 10\n",
    "#         outputs = model(images)\n",
    "\n",
    "#         # Calculate Loss: softmax --> cross entropy loss\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Getting gradients w.r.t. parameters\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Updating parameters\n",
    "#         optimizer.step()\n",
    "\n",
    "#         iter += 1\n",
    "\n",
    "#         if iter % 500 == 0:\n",
    "#             model.eval()\n",
    "#             # Calculate Accuracy         \n",
    "#             correct = 0\n",
    "#             total = 0\n",
    "#             # Iterate through test dataset\n",
    "#             for images, labels in test_loader:\n",
    "#                 # Load images to a Torch tensors with gradient accumulation abilities\n",
    "#                 images = images.view(-1, seq_dim, input_dim)\n",
    "\n",
    "#                 # Forward pass only to get logits/output\n",
    "#                 outputs = model(images)\n",
    "\n",
    "#                 # Get predictions from the maximum value\n",
    "#                 _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "#                 # Total number of labels\n",
    "#                 total += labels.size(0)\n",
    "\n",
    "#                 # Total correct predictions\n",
    "#                 correct += (predicted == labels).sum()\n",
    "\n",
    "#             accuracy = 100 * correct / total\n",
    "\n",
    "#             # Print Loss\n",
    "#             print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea1b8ba-e3ad-4478-854d-35f2ad4fead9",
   "metadata": {},
   "source": [
    "# Using SK-Learn's Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "676fbce6-d85a-47a8-9d3f-19d47ad2f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "class batteriesDSnp(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        \n",
    "        self.data = torch.tensor(data)\n",
    "        self.targets = torch.tensor(targets)\n",
    "                \n",
    "        return\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        current_sample = self.data[index,:]\n",
    "        current_targets = self.targets[index]\n",
    "        return {\n",
    "            \"x\": torch.tensor(current_sample, dtype = torch.float),\n",
    "            \"y\": torch.tensor(current_targets, dtype = torch.float)\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e7dc4eb9-51ad-4f8f-8d35-1b1ad5ee8c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Normalize Data\n",
    "# from sklearn import preprocessing\n",
    "\n",
    "# scaler = preprocessing.MinMaxScaler()\n",
    "# df_data_std = scaler.fit_transform(df_data)\n",
    "# df_data_std = pd.DataFrame(df_data_std, columns =features)\n",
    "# df_data_std.head()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_data, df_targets, test_size=0.33, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_sk_stdzd = (scaler.transform(X_train))\n",
    "X_test_sk_stdzd = (scaler.transform(X_test))\n",
    "\n",
    "train_dataset =  batteriesDSnp(X_train_sk_stdzd, y_train.to_numpy())\n",
    "test_dataset = batteriesDSnp(X_test_sk_stdzd, y_train.to_numpy())\n",
    "\n",
    "\n",
    "# Create loaders\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "#num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = 5 #int(num_epochs)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True , drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "5d937750-4fad-4242-a297-0f3967bb6019",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # Building your RNN\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, input_dim)\n",
    "        # batch_dim = number of samples per batch\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
    "\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        # (layer_dim, batch_size, hidden_dim)\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # We need to detach the hidden state to prevent exploding/vanishing gradients\n",
    "        # This is part of truncated backpropagation through time (BPTT)\n",
    "        out, hn = self.rnn(x, h0.detach())\n",
    "\n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 28, 10\n",
    "        # out[:, -1, :] --> 100, 10 --> just want last time step hidden states! \n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out.size() --> 100, 10\n",
    "        return out\n",
    "    \n",
    "#Hyperparameters\n",
    "\n",
    "input_dim = len(features)\n",
    "hidden_dim = 100\n",
    "layer_dim = 1\n",
    "output_dim = 1\n",
    "\n",
    "model = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89be5262-9a37-4b18-80db-a1d767e58f12",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 7RNN_SK: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "4e892b52-5ad4-4951-a0f5-a03d84d6d500",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fes33\\AppData\\Local\\Temp\\ipykernel_5252\\2367303398.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"x\": torch.tensor(current_sample, dtype = torch.float),\n",
      "C:\\Users\\fes33\\AppData\\Local\\Temp\\ipykernel_5252\\2367303398.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"y\": torch.tensor(current_targets, dtype = torch.float)\n",
      "D:\\Users\\fes33\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:520: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4131618.0\n",
      "1877949.875\n",
      "765851795456.0\n",
      "2.552666890950177e+29\n",
      "inf\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "# Number of steps to unroll\n",
    "seq_dim = 1  \n",
    "shape = [batch_size,1,input_dim]\n",
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, trainbatch in enumerate(train_loader):\n",
    "        model.train()\n",
    "        # Load images as tensors with gradient accumulation abilities\n",
    "        #images = images.view(-1, seq_dim, input_dim).requires_grad_()\n",
    "        trainbatch['x'] = torch.reshape(trainbatch['x'], shape)\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        # outputs.size() --> 100, 10\n",
    "        outputs = model(trainbatch['x'])\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, trainbatch['y'])\n",
    "        \n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        #print(i)\n",
    "        \n",
    "        iter += 1\n",
    "        print(loss.item())\n",
    "#         if iter % 50 == 0:\n",
    "#             model.eval()\n",
    "#             # Calculate Accuracy         \n",
    "#             correct = 0\n",
    "#             total = 0\n",
    "#             # Iterate through test dataset\n",
    "#             for testbatch in test_loader: \n",
    "#                 # Load images to a Torch tensors with gradient accumulation abilities\n",
    "#                 #images = images.view(-1, seq_dim, input_dim)\n",
    "#                 testbatch['x']= torch.reshape(trainbatch['x'], shape)\n",
    "\n",
    "#                 # Forward pass only to get logits/output\n",
    "#                 outputs = model(testbatch['x'])\n",
    "\n",
    "#                 # Get predictions from the maximum value\n",
    "#                 _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "#                 # Total number of labels\n",
    "#                 total += testbatch['y'].size(0)\n",
    "\n",
    "#                 # Total correct predictions\n",
    "#                 correct += (predicted == testbatch['y']).sum()\n",
    "\n",
    "#             accuracy = 100 * correct / total\n",
    "\n",
    "#             # Print Loss\n",
    "#             print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5022260-b655-4c4e-8e95-3480ed79fce0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1609aa23-c4c4-4c70-8186-75893de6add9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Step 3: Create LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a03170-809d-46d9-8df0-e016f8f9a26c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # Building your LSTM\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # 28 time steps\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 28, 100\n",
    "        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out.size() --> 100, 10\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fbd4c9-05e5-421c-8b7f-b23f325b5f29",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Instantiate our LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985e5d6a-46d6-4672-90e8-47b1c0561175",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28\n",
    "hidden_dim = 100\n",
    "layer_dim = 1\n",
    "output_dim = 10\n",
    "\n",
    "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab7cbd5-2374-4db2-95f7-9051254a24c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
